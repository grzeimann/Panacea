{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Panacea Documentation Welcome to the Panacea docs. This site organizes the previous long README into topic-focused pages. :hidden: :maxdepth: 2 getting-started/installation.md getting-started/quickstart.md tacc/overview.md tacc/running.md user-guide/cli.md user-guide/configuration.md user-guide/examples.md migration/sphinx.md data-products/overview.md algorithms/overview.md community/contributing.md community/code_of_conduct.md development/dev-quickstart.md development/tests-overview.md release_info/changelog.md citation/citation.md api/index.md faq.md guide.md Getting started Installation Quickstart TACC (HET users) Overview Running on TACC User Guide CLI Usage Configuration Data Products Algorithms (high-level) API Reference Community Contributing Code of Conduct Release Info Changelog Citation FAQ See also the project README for a short overview on GitHub: https://github.com/grzeimann/Panacea#readme See also: Quickstart , TACC Overview","title":"Home"},{"location":"#panacea-documentation","text":"Welcome to the Panacea docs. This site organizes the previous long README into topic-focused pages. :hidden: :maxdepth: 2 getting-started/installation.md getting-started/quickstart.md tacc/overview.md tacc/running.md user-guide/cli.md user-guide/configuration.md user-guide/examples.md migration/sphinx.md data-products/overview.md algorithms/overview.md community/contributing.md community/code_of_conduct.md development/dev-quickstart.md development/tests-overview.md release_info/changelog.md citation/citation.md api/index.md faq.md guide.md Getting started Installation Quickstart TACC (HET users) Overview Running on TACC User Guide CLI Usage Configuration Data Products Algorithms (high-level) API Reference Community Contributing Code of Conduct Release Info Changelog Citation FAQ See also the project README for a short overview on GitHub: https://github.com/grzeimann/Panacea#readme See also: Quickstart , TACC Overview","title":"Panacea Documentation"},{"location":"faq/","text":"Frequently Asked Questions This page collects common questions about Panacea usage, calibration, and outputs. What wavelength units do Panacea products use? Wavelengths are in vacuum Angstroms (\u00c5). Spectra are rectified onto a common vacuum wavelength grid per channel during extraction. How is the flux calibration handled? Do I need a nightly standard star? By default, Panacea applies a packaged average response curve for each LRS2 channel (uv, orange, red, farred). It does not require or use a standard star from the same night. Why this works: The Hobby\u2013Eberly Telescope (HET) operates at a fixed altitude and typically observes at a near-constant airmass. This stability makes a median \u201cstandard response\u201d effective for quicklook calibration. Practical implications: The response places spectra on a consistent relative flux scale suitable for quicklook science and comparisons across nights. The initial calibration is good to 10% across wavelengths in relative calibration and ~20% in absolute. If you need per-night absolute spectrophotometry, you can derive a bespoke response outside of the quicklook path and apply it post\u2011hoc. LRS2Multi (below) is great for secondary calibration Are the outputs single-channel or combined across channels? Panacea\u2019s reduction products are per channel (one of uv, orange, red, farred) and are written separately. To stitch/combine channels into a single spectrum or cube, use LRS2Multi: LRS2Multi repository: https://github.com/grzeimann/LRS2Multi Can I select which wavelengths are emphasized in the collapsed image? Yes. Use the CLI options: --central_wave: Center wavelength (\u00c5) for the collapsed image window. --wavelength_bin: Half-width (\u00c5) around the center used for collapsing. See the CLI docs for details and examples. Where can I learn more about configuration files and defaults? See the Configuration guide, which lists the packaged line lists, DAR tables, response curves, and geometry resources. See also: - CLI Usage - Configuration - Data Products - Installation","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"This page collects common questions about Panacea usage, calibration, and outputs.","title":"Frequently Asked Questions"},{"location":"faq/#what-wavelength-units-do-panacea-products-use","text":"Wavelengths are in vacuum Angstroms (\u00c5). Spectra are rectified onto a common vacuum wavelength grid per channel during extraction.","title":"What wavelength units do Panacea products use?"},{"location":"faq/#how-is-the-flux-calibration-handled-do-i-need-a-nightly-standard-star","text":"By default, Panacea applies a packaged average response curve for each LRS2 channel (uv, orange, red, farred). It does not require or use a standard star from the same night. Why this works: The Hobby\u2013Eberly Telescope (HET) operates at a fixed altitude and typically observes at a near-constant airmass. This stability makes a median \u201cstandard response\u201d effective for quicklook calibration. Practical implications: The response places spectra on a consistent relative flux scale suitable for quicklook science and comparisons across nights. The initial calibration is good to 10% across wavelengths in relative calibration and ~20% in absolute. If you need per-night absolute spectrophotometry, you can derive a bespoke response outside of the quicklook path and apply it post\u2011hoc. LRS2Multi (below) is great for secondary calibration","title":"How is the flux calibration handled? Do I need a nightly standard star?"},{"location":"faq/#are-the-outputs-single-channel-or-combined-across-channels","text":"Panacea\u2019s reduction products are per channel (one of uv, orange, red, farred) and are written separately. To stitch/combine channels into a single spectrum or cube, use LRS2Multi: LRS2Multi repository: https://github.com/grzeimann/LRS2Multi","title":"Are the outputs single-channel or combined across channels?"},{"location":"faq/#can-i-select-which-wavelengths-are-emphasized-in-the-collapsed-image","text":"Yes. Use the CLI options: --central_wave: Center wavelength (\u00c5) for the collapsed image window. --wavelength_bin: Half-width (\u00c5) around the center used for collapsing. See the CLI docs for details and examples.","title":"Can I select which wavelengths are emphasized in the collapsed image?"},{"location":"faq/#where-can-i-learn-more-about-configuration-files-and-defaults","text":"See the Configuration guide, which lists the packaged line lists, DAR tables, response curves, and geometry resources. See also: - CLI Usage - Configuration - Data Products - Installation","title":"Where can I learn more about configuration files and defaults?"},{"location":"guide/","text":"Documentation Guide This folder contains the Panacea documentation split from the long README into topic-focused pages. All links are relative so the docs render correctly on GitHub and remain compatible with a future docs site (e.g., MkDocs). Structure index.md \u2014 landing page and entry points getting-started/ installation.md \u2014 local install and dependencies quickstart.md \u2014 first-run CLI examples tacc/ overview.md \u2014 accounts, access, and where to find products on TACC running.md \u2014 interactive and batch usage on TACC user-guide/ cli.md \u2014 panacea-lrs2 CLI options and examples configuration.md \u2014 config files, responses, fplane, line lists data-products/ overview.md \u2014 definitions and directory layout of outputs algorithms/ overview.md \u2014 high-level algorithms and references citation/ citation.md \u2014 how to cite Panacea (see CITATION.cff at the repository root: https://github.com/grzeimann/Panacea/blob/HEAD/CITATION.cff) community/ contributing.md \u2014 contribution guide code_of_conduct.md \u2014 community standards release_info/ changelog.md \u2014 link to ../../CHANGELOG.md images/ \u2014 images referenced from docs and README Conventions One H1 (#) per page. Keep pages focused; prefer new pages when content gets long. Use relative links (./ or ../) within docs/. Add a short \"See also\" section at the bottom to connect related pages. Add language specifiers to fenced code blocks ( bash, text, etc.). Linking and compatibility Relative links are intentionally used so the same Markdown pages work on: GitHub (links resolve relative to the current file) MkDocs (links resolve within the docs/ tree) Sphinx, when using MyST Markdown (links resolve as document references as long as the target files are part of the Sphinx source) For Sphinx adoption without converting formats, see migration notes: ./migration/sphinx.md Editing and contributions Keep filenames lowercase-with-hyphens to preserve URL stability. When adding a new page, link it from index.md and include a short \"See also\". Prefer incremental PRs that add or move a small number of pages at a time. If you change paths or filenames, verify all inbound links are updated. See also: Home , Installation , TACC Overview Building the documentation You can build the docs in two ways. Use Sphinx for the full API reference (autodoc), or MkDocs for a quick brochure site. Both can be run locally. Option A: Build with Sphinx (recommended for API pages) This uses docs/conf.py and will render the API reference via autodoc. Output goes to _build/html. 1) Create/activate an environment and install doc deps - Using pip only: - python -m pip install --upgrade pip - pip install .[docs] - Using conda (optional) then pip for extras: - conda env update -f environment.yml # creates/updates env - python -m pip install --upgrade pip - pip install .[docs] 2) Build HTML into _build/html - Clean old build (optional): - rm -rf _build/html - Build: - sphinx-build -b html docs _build/html - If you run into stale caches, use: sphinx-build -E -a -b html docs _build/html 3) View locally - Open _build/html/index.html in your browser. Notes - docs/conf.py already adds ../src to sys.path, so autodoc can import panacea from a source checkout. If import errors persist, install the package in editable mode: pip install -e . - If you prefer a theme like sphinx_rtd_theme, install it (auto\u2011detected in conf.py when available): pip install sphinx-rtd-theme. Option B: Build with MkDocs (quick brochure site) This will build the navigation defined in mkdocs.yml. Sphinx-only blocks like ```{eval-rst} with autodoc will not execute under MkDocs; they render as code blocks. 1) Install MkDocs - python -m pip install mkdocs - (Optional) Add a theme: pip install mkdocs-material 2) Build or serve - Build to the default site/ directory: - mkdocs build - Or build to _build/mkdocs: - mkdocs build -d _build/mkdocs - Live-reload preview at http://127.0.0.1:8000/: - mkdocs serve 3) View locally - Open site/index.html (or _build/mkdocs/index.html if you set -d). Common issues and fixes - ImportError during Sphinx autodoc: Ensure the environment is active and either rely on docs/conf.py\u2019s sys.path tweak or run pip install -e . to make panacea importable. - Extensions missing: If myst-parser or furo aren\u2019t found, make sure you ran pip install .[docs]. - Stale API pages: Clean the build or pass -E -a to sphinx-build as shown above.","title":"Documentation Guide"},{"location":"guide/#documentation-guide","text":"This folder contains the Panacea documentation split from the long README into topic-focused pages. All links are relative so the docs render correctly on GitHub and remain compatible with a future docs site (e.g., MkDocs).","title":"Documentation Guide"},{"location":"guide/#structure","text":"index.md \u2014 landing page and entry points getting-started/ installation.md \u2014 local install and dependencies quickstart.md \u2014 first-run CLI examples tacc/ overview.md \u2014 accounts, access, and where to find products on TACC running.md \u2014 interactive and batch usage on TACC user-guide/ cli.md \u2014 panacea-lrs2 CLI options and examples configuration.md \u2014 config files, responses, fplane, line lists data-products/ overview.md \u2014 definitions and directory layout of outputs algorithms/ overview.md \u2014 high-level algorithms and references citation/ citation.md \u2014 how to cite Panacea (see CITATION.cff at the repository root: https://github.com/grzeimann/Panacea/blob/HEAD/CITATION.cff) community/ contributing.md \u2014 contribution guide code_of_conduct.md \u2014 community standards release_info/ changelog.md \u2014 link to ../../CHANGELOG.md images/ \u2014 images referenced from docs and README","title":"Structure"},{"location":"guide/#conventions","text":"One H1 (#) per page. Keep pages focused; prefer new pages when content gets long. Use relative links (./ or ../) within docs/. Add a short \"See also\" section at the bottom to connect related pages. Add language specifiers to fenced code blocks ( bash, text, etc.).","title":"Conventions"},{"location":"guide/#linking-and-compatibility","text":"Relative links are intentionally used so the same Markdown pages work on: GitHub (links resolve relative to the current file) MkDocs (links resolve within the docs/ tree) Sphinx, when using MyST Markdown (links resolve as document references as long as the target files are part of the Sphinx source) For Sphinx adoption without converting formats, see migration notes: ./migration/sphinx.md","title":"Linking and compatibility"},{"location":"guide/#editing-and-contributions","text":"Keep filenames lowercase-with-hyphens to preserve URL stability. When adding a new page, link it from index.md and include a short \"See also\". Prefer incremental PRs that add or move a small number of pages at a time. If you change paths or filenames, verify all inbound links are updated. See also: Home , Installation , TACC Overview","title":"Editing and contributions"},{"location":"guide/#building-the-documentation","text":"You can build the docs in two ways. Use Sphinx for the full API reference (autodoc), or MkDocs for a quick brochure site. Both can be run locally.","title":"Building the documentation"},{"location":"guide/#option-a-build-with-sphinx-recommended-for-api-pages","text":"This uses docs/conf.py and will render the API reference via autodoc. Output goes to _build/html. 1) Create/activate an environment and install doc deps - Using pip only: - python -m pip install --upgrade pip - pip install .[docs] - Using conda (optional) then pip for extras: - conda env update -f environment.yml # creates/updates env - python -m pip install --upgrade pip - pip install .[docs] 2) Build HTML into _build/html - Clean old build (optional): - rm -rf _build/html - Build: - sphinx-build -b html docs _build/html - If you run into stale caches, use: sphinx-build -E -a -b html docs _build/html 3) View locally - Open _build/html/index.html in your browser. Notes - docs/conf.py already adds ../src to sys.path, so autodoc can import panacea from a source checkout. If import errors persist, install the package in editable mode: pip install -e . - If you prefer a theme like sphinx_rtd_theme, install it (auto\u2011detected in conf.py when available): pip install sphinx-rtd-theme.","title":"Option A: Build with Sphinx (recommended for API pages)"},{"location":"guide/#option-b-build-with-mkdocs-quick-brochure-site","text":"This will build the navigation defined in mkdocs.yml. Sphinx-only blocks like ```{eval-rst} with autodoc will not execute under MkDocs; they render as code blocks. 1) Install MkDocs - python -m pip install mkdocs - (Optional) Add a theme: pip install mkdocs-material 2) Build or serve - Build to the default site/ directory: - mkdocs build - Or build to _build/mkdocs: - mkdocs build -d _build/mkdocs - Live-reload preview at http://127.0.0.1:8000/: - mkdocs serve 3) View locally - Open site/index.html (or _build/mkdocs/index.html if you set -d). Common issues and fixes - ImportError during Sphinx autodoc: Ensure the environment is active and either rely on docs/conf.py\u2019s sys.path tweak or run pip install -e . to make panacea importable. - Extensions missing: If myst-parser or furo aren\u2019t found, make sure you ran pip install .[docs]. - Stale API pages: Clean the build or pass -E -a to sphinx-build as shown above.","title":"Option B: Build with MkDocs (quick brochure site)"},{"location":"algorithms/overview/","text":"Algorithms Overview This page provides a high-level overview of the reduction steps implemented in Panacea. Detailed implementation notes live in the code; this page is intended as a conceptual guide. LRS2 Layout LRS2 provides integral-field-unit (IFU) spectroscopy using 280 0.6\"-diameter lenslets that cover a 12\"\u00d76\" field of view (FOV) on the sky. LRS2 is composed of two arms: blue (LRS2-B) and red (LRS2-R). The LRS2-B arm employs a dichroic beamsplitter to send light simultaneously into two spectrograph units: the \"UV\" channel (covering 3640\u20134645 \u00c5 at resolving power 1910), and the \"Orange\" channel (covering 4635\u20136950 \u00c5 at resolving power 1140). The LRS2-R is also split into two spectrograph units: the \"Red\" channel (covering 6450\u20138450 \u00c5 at resolving power 1760), and the \"Far Red\" channel (covering 8250\u201310500 \u00c5 at resolving power 1920). 1. Overscan and Bias Subtraction Each CCD frame is first corrected for electronic bias by fitting and subtracting the overscan level independently for each amplifier. Panacea excludes the first column of the overscan region and uses the remaining 31 or 63 pixels per row (depending on binning) to determine the row-by-row bias pedestal. Residual two-dimensional bias structure is removed using a master bias, constructed from ~100 bias frames across several nights to achieve high S/N while minimizing temporal drift. This process removes amplifier offsets and pattern noise before flat-fielding. 2. Fiber Tracing and Extraction Using the master flat, Panacea identifies and traces each fiber\u2019s centroid along the dispersion axis. The fiber positions are modeled as smooth polynomials describing their curvature across the CCD. 1D spectra are extracted following the \"flat-relative optimal extraction\" (Zechmeister et al. 2014, A&A, 561, A59), in which the science frame is divided by a normalized flat before extraction to mitigate pixel-response variations and achieve optimal weighting. Cosmic rays are flagged and rejected using a similar algorithm to that of Malte Tewes and Pieter van Dokkum. 3. Wavelength Calibration Wavelength solutions are derived from arc-lamp exposures (Hg, Cd, Ne, Ar, Fe) taken near in time to the science frames. Line identifications for each fiber are matched to laboratory wavelengths, and Panacea fits a polynomial dispersion solution (typically 3rd order). These solutions are refined per amplifier and per spectrograph channel, ensuring internal consistency between the LRS2 arms. The wavelength calibration is applied to all extracted spectra and rectified data products. 4. Flat-Fielding and Fiber Normalization Twilight or internal flat exposures are used to model the fiber profiles and correct both pixel-to-pixel and fiber-to-fiber throughput variations. The normalized flat defines a fiber profile model that traces the flux distribution of each fiber across the CCD. Panacea supports both fiber-to-fiber (FTF) correction and optional use_flat flags for selecting twilight versus internal flats. Flat-fielding also corrects for wavelength-dependent sensitivity variations within each fiber. 5. Sky Subtraction Panacea constructs a two-dimensional sky model for each exposure and subtracts it from the rectified fiber spectra. The approach is empirical and robust against faint sources, using only sky-dominated pixels to model spatial and spectral structure. Algorithm overview 1. Identify sky fibers and build a median template - The median flux is computed for each fiber, and those with the lowest background levels are flagged as \u201csky-like.\u201d - These fibers are used to form an initial 1D median sky template across wavelength. - A smoothed continuum version of this template is created by convolving with a broad Gaussian kernel. - Significant deviations between the original and smoothed spectra identify bright sky-line regions. 2. Per-fiber scaling of the sky template - For each fiber, Panacea determines a scalar that best scales the global sky template to match the fiber\u2019s data at sky-line wavelengths. - A grid of trial scale factors (0.7\u20131.3) is tested, and the best value minimizes residuals between the observed and template spectra. - This yields a set of per-fiber scale factors that represent spatial variation of the sky intensity across the IFU. 3. Normalize by amplifier and fit a smooth 2D field model - Since LRS2 channels are split into two amplifiers (140 fibers per amplifier), Panacea normalizes scale factors within each arm separately to remove global offsets. - A 2D polynomial (degree 2) is then fit to the normalized scale factors as a function of the fibers\u2019 focal-plane coordinates (x, y). - Outliers more than twice the median absolute deviation from the fit are rejected, and the model is refit for stability. 4. Construct the final fiber-resolved sky model - The initial 1D sky template is multiplied by the fitted 2D surface to form a full fiber-by-fiber sky model. - This reproduces both spectral and spatial variations in the background illumination. 5. Subtract and propagate - The modeled sky spectrum is subtracted from each fiber\u2019s rectified spectrum. - Errors are propagated consistently, and flagged bad fibers or masked pixels are set to zero. - The resulting arrays include the observed spectra, modeled sky, and sky-subtracted spectra for downstream analysis. Notes and diagnostics - The method emphasizes sky-dominated fibers and bright sky lines, avoiding bias from object flux. - The polynomial surface ensures smooth spatial variation across the IFU. - Panacea\u2019s multi-extension FITS outputs include both the rectified sky model and the sky-subtracted spectra for quality assurance. 6. Relative Flux Calibration An initial relative flux calibration is computed using the default LRS2 response curves derived from spectrophotometric standard stars. Panacea scales these curves based on estimates of telescope illumination and exposure throughput, as measured from guide-camera photometry of field stars. This process compensates for night-to-night transparency and mirror reflectivity variations, yielding consistent relative flux scales between nights and between channels. 7. Final Data Products and Flow Diagram Each reduction produces multi-extension FITS files containing: - Fiber-extracted, wavelength- and flux-calibrated spectra - Sky and sky-subtracted spectra - Error frames and response curves - Fiber position tables (IFU, focal-plane, and sky coordinates) - Collapsed images and atmospheric differential refraction (ADR) tables - Rectified and unrectified 2D spectra for visualization and diagnostics These products form the complete foundation for LRS2 science analysis and are automatically archived and accessible via TACC. graph TD A[Raw CCD Frames] --> B[Overscan & Bias Subtraction] B --> C[Fiber Tracing & Extraction] C --> D[Wavelength Calibration] D --> E[Flat-Fielding & Fiber Normalization] E --> F[Sky Subtraction] F --> G[Relative Flux Calibration] G --> H[Science Data Products] H --> I[Multi-extension FITS files: spectra, sky, error, ADR, positions] style A fill:#d3e8ff,stroke:#003366,stroke-width:1px style H fill:#d3ffd3,stroke:#006600,stroke-width:1px See also: Data Products , Configuration","title":"Algorithms"},{"location":"algorithms/overview/#algorithms-overview","text":"This page provides a high-level overview of the reduction steps implemented in Panacea. Detailed implementation notes live in the code; this page is intended as a conceptual guide.","title":"Algorithms Overview"},{"location":"algorithms/overview/#lrs2-layout","text":"LRS2 provides integral-field-unit (IFU) spectroscopy using 280 0.6\"-diameter lenslets that cover a 12\"\u00d76\" field of view (FOV) on the sky. LRS2 is composed of two arms: blue (LRS2-B) and red (LRS2-R). The LRS2-B arm employs a dichroic beamsplitter to send light simultaneously into two spectrograph units: the \"UV\" channel (covering 3640\u20134645 \u00c5 at resolving power 1910), and the \"Orange\" channel (covering 4635\u20136950 \u00c5 at resolving power 1140). The LRS2-R is also split into two spectrograph units: the \"Red\" channel (covering 6450\u20138450 \u00c5 at resolving power 1760), and the \"Far Red\" channel (covering 8250\u201310500 \u00c5 at resolving power 1920).","title":"LRS2 Layout"},{"location":"algorithms/overview/#1-overscan-and-bias-subtraction","text":"Each CCD frame is first corrected for electronic bias by fitting and subtracting the overscan level independently for each amplifier. Panacea excludes the first column of the overscan region and uses the remaining 31 or 63 pixels per row (depending on binning) to determine the row-by-row bias pedestal. Residual two-dimensional bias structure is removed using a master bias, constructed from ~100 bias frames across several nights to achieve high S/N while minimizing temporal drift. This process removes amplifier offsets and pattern noise before flat-fielding.","title":"1. Overscan and Bias Subtraction"},{"location":"algorithms/overview/#2-fiber-tracing-and-extraction","text":"Using the master flat, Panacea identifies and traces each fiber\u2019s centroid along the dispersion axis. The fiber positions are modeled as smooth polynomials describing their curvature across the CCD. 1D spectra are extracted following the \"flat-relative optimal extraction\" (Zechmeister et al. 2014, A&A, 561, A59), in which the science frame is divided by a normalized flat before extraction to mitigate pixel-response variations and achieve optimal weighting. Cosmic rays are flagged and rejected using a similar algorithm to that of Malte Tewes and Pieter van Dokkum.","title":"2. Fiber Tracing and Extraction"},{"location":"algorithms/overview/#3-wavelength-calibration","text":"Wavelength solutions are derived from arc-lamp exposures (Hg, Cd, Ne, Ar, Fe) taken near in time to the science frames. Line identifications for each fiber are matched to laboratory wavelengths, and Panacea fits a polynomial dispersion solution (typically 3rd order). These solutions are refined per amplifier and per spectrograph channel, ensuring internal consistency between the LRS2 arms. The wavelength calibration is applied to all extracted spectra and rectified data products.","title":"3. Wavelength Calibration"},{"location":"algorithms/overview/#4-flat-fielding-and-fiber-normalization","text":"Twilight or internal flat exposures are used to model the fiber profiles and correct both pixel-to-pixel and fiber-to-fiber throughput variations. The normalized flat defines a fiber profile model that traces the flux distribution of each fiber across the CCD. Panacea supports both fiber-to-fiber (FTF) correction and optional use_flat flags for selecting twilight versus internal flats. Flat-fielding also corrects for wavelength-dependent sensitivity variations within each fiber.","title":"4. Flat-Fielding and Fiber Normalization"},{"location":"algorithms/overview/#5-sky-subtraction","text":"Panacea constructs a two-dimensional sky model for each exposure and subtracts it from the rectified fiber spectra. The approach is empirical and robust against faint sources, using only sky-dominated pixels to model spatial and spectral structure. Algorithm overview 1. Identify sky fibers and build a median template - The median flux is computed for each fiber, and those with the lowest background levels are flagged as \u201csky-like.\u201d - These fibers are used to form an initial 1D median sky template across wavelength. - A smoothed continuum version of this template is created by convolving with a broad Gaussian kernel. - Significant deviations between the original and smoothed spectra identify bright sky-line regions. 2. Per-fiber scaling of the sky template - For each fiber, Panacea determines a scalar that best scales the global sky template to match the fiber\u2019s data at sky-line wavelengths. - A grid of trial scale factors (0.7\u20131.3) is tested, and the best value minimizes residuals between the observed and template spectra. - This yields a set of per-fiber scale factors that represent spatial variation of the sky intensity across the IFU. 3. Normalize by amplifier and fit a smooth 2D field model - Since LRS2 channels are split into two amplifiers (140 fibers per amplifier), Panacea normalizes scale factors within each arm separately to remove global offsets. - A 2D polynomial (degree 2) is then fit to the normalized scale factors as a function of the fibers\u2019 focal-plane coordinates (x, y). - Outliers more than twice the median absolute deviation from the fit are rejected, and the model is refit for stability. 4. Construct the final fiber-resolved sky model - The initial 1D sky template is multiplied by the fitted 2D surface to form a full fiber-by-fiber sky model. - This reproduces both spectral and spatial variations in the background illumination. 5. Subtract and propagate - The modeled sky spectrum is subtracted from each fiber\u2019s rectified spectrum. - Errors are propagated consistently, and flagged bad fibers or masked pixels are set to zero. - The resulting arrays include the observed spectra, modeled sky, and sky-subtracted spectra for downstream analysis. Notes and diagnostics - The method emphasizes sky-dominated fibers and bright sky lines, avoiding bias from object flux. - The polynomial surface ensures smooth spatial variation across the IFU. - Panacea\u2019s multi-extension FITS outputs include both the rectified sky model and the sky-subtracted spectra for quality assurance.","title":"5. Sky Subtraction"},{"location":"algorithms/overview/#6-relative-flux-calibration","text":"An initial relative flux calibration is computed using the default LRS2 response curves derived from spectrophotometric standard stars. Panacea scales these curves based on estimates of telescope illumination and exposure throughput, as measured from guide-camera photometry of field stars. This process compensates for night-to-night transparency and mirror reflectivity variations, yielding consistent relative flux scales between nights and between channels.","title":"6. Relative Flux Calibration"},{"location":"algorithms/overview/#7-final-data-products-and-flow-diagram","text":"Each reduction produces multi-extension FITS files containing: - Fiber-extracted, wavelength- and flux-calibrated spectra - Sky and sky-subtracted spectra - Error frames and response curves - Fiber position tables (IFU, focal-plane, and sky coordinates) - Collapsed images and atmospheric differential refraction (ADR) tables - Rectified and unrectified 2D spectra for visualization and diagnostics These products form the complete foundation for LRS2 science analysis and are automatically archived and accessible via TACC. graph TD A[Raw CCD Frames] --> B[Overscan & Bias Subtraction] B --> C[Fiber Tracing & Extraction] C --> D[Wavelength Calibration] D --> E[Flat-Fielding & Fiber Normalization] E --> F[Sky Subtraction] F --> G[Relative Flux Calibration] G --> H[Science Data Products] H --> I[Multi-extension FITS files: spectra, sky, error, ADR, positions] style A fill:#d3e8ff,stroke:#003366,stroke-width:1px style H fill:#d3ffd3,stroke:#006600,stroke-width:1px See also: Data Products , Configuration","title":"7. Final Data Products and Flow Diagram"},{"location":"api/","text":"API Reference This section documents the public Python API of the Panacea package. The index below is generated by Sphinx autodoc/autosummary from the source under src/ . .. automodule:: panacea :noindex: .. currentmodule:: panacea .. toctree:: :maxdepth: 1 cli astrometry ccd sky wavelength fiber trace routine io utils Notes - Ensure your environment can import the package. When building from a source checkout, docs/conf.py already adds ../src to sys.path so autodoc can import panacea without installation. - If you prefer to install, use: pip install -e .[dev] in your environment. See also: Configuration , CLI Usage","title":"Overview"},{"location":"api/#api-reference","text":"This section documents the public Python API of the Panacea package. The index below is generated by Sphinx autodoc/autosummary from the source under src/ . .. automodule:: panacea :noindex: .. currentmodule:: panacea .. toctree:: :maxdepth: 1 cli astrometry ccd sky wavelength fiber trace routine io utils Notes - Ensure your environment can import the package. When building from a source checkout, docs/conf.py already adds ../src to sys.path so autodoc can import panacea without installation. - If you prefer to install, use: pip install -e .[dev] in your environment. See also: Configuration , CLI Usage","title":"API Reference"},{"location":"api/astrometry/","text":"panacea.astrometry Astrometry utilities for converting between IFU slot coordinates and sky coordinates. .. automodule:: panacea.astrometry :members: Astrometry, IFU, FPlane :undoc-members: :show-inheritance: Example - Initialize an Astrometry model and query an IFU slot position: from panacea.astrometry import Astrometry astro = Astrometry(ra0=150.0, dec0=2.2, pa=0.0, x0=0.0, y0=0.0) ra, dec = astro.get_ifuslot_ra_dec('054') print(ra, dec)","title":"Astrometry"},{"location":"api/astrometry/#panaceaastrometry","text":"Astrometry utilities for converting between IFU slot coordinates and sky coordinates. .. automodule:: panacea.astrometry :members: Astrometry, IFU, FPlane :undoc-members: :show-inheritance: Example - Initialize an Astrometry model and query an IFU slot position: from panacea.astrometry import Astrometry astro = Astrometry(ra0=150.0, dec0=2.2, pa=0.0, x0=0.0, y0=0.0) ra, dec = astro.get_ifuslot_ra_dec('054') print(ra, dec)","title":"panacea.astrometry"},{"location":"api/ccd/","text":"panacea.ccd .. automodule:: panacea.ccd :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.ccd"},{"location":"api/ccd/#panaceaccd","text":".. automodule:: panacea.ccd :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.ccd"},{"location":"api/cli/","text":"panacea.cli .. automodule:: panacea.cli :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.cli"},{"location":"api/cli/#panaceacli","text":".. automodule:: panacea.cli :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.cli"},{"location":"api/fiber/","text":"panacea.fiber .. automodule:: panacea.fiber :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.fiber"},{"location":"api/fiber/#panaceafiber","text":".. automodule:: panacea.fiber :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.fiber"},{"location":"api/io/","text":"panacea.io .. automodule:: panacea.io :members: :undoc-members: :show-inheritance: :inherited-members:","title":"I/O"},{"location":"api/io/#panaceaio","text":".. automodule:: panacea.io :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.io"},{"location":"api/routine/","text":"panacea.routine High-level routines used in calibration and source extraction. .. automodule:: panacea.routine :members: fit_response_cont, get_response, extract_source, find_source :undoc-members: :show-inheritance: Examples - Estimate a smooth continuum for a noisy spectrum: import numpy as np from panacea.routine import fit_response_cont wv = np.linspace(4000, 5000, 501) sky = 1.0 + 0.001*(wv - wv.mean()) sky[250] += 0.8 # narrow emission feature cont = fit_response_cont(wv, sky, fil_len=21) Compute a response vector from a recognized standard (returns None if not found): import numpy as np from panacea.routine import get_response wv = np.linspace(4000, 5000, 501) sky = 1.0 + 0.001*(wv - wv.mean()) resp = get_response('HZ_44', wv, sky, 'red') if resp is not None: calibrated = sky * resp","title":"Routine"},{"location":"api/routine/#panacearoutine","text":"High-level routines used in calibration and source extraction. .. automodule:: panacea.routine :members: fit_response_cont, get_response, extract_source, find_source :undoc-members: :show-inheritance: Examples - Estimate a smooth continuum for a noisy spectrum: import numpy as np from panacea.routine import fit_response_cont wv = np.linspace(4000, 5000, 501) sky = 1.0 + 0.001*(wv - wv.mean()) sky[250] += 0.8 # narrow emission feature cont = fit_response_cont(wv, sky, fil_len=21) Compute a response vector from a recognized standard (returns None if not found): import numpy as np from panacea.routine import get_response wv = np.linspace(4000, 5000, 501) sky = 1.0 + 0.001*(wv - wv.mean()) resp = get_response('HZ_44', wv, sky, 'red') if resp is not None: calibrated = sky * resp","title":"panacea.routine"},{"location":"api/sky/","text":"panacea.sky .. automodule:: panacea.sky :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.sky"},{"location":"api/sky/#panaceasky","text":".. automodule:: panacea.sky :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.sky"},{"location":"api/trace/","text":"panacea.trace .. automodule:: panacea.trace :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.trace"},{"location":"api/trace/#panaceatrace","text":".. automodule:: panacea.trace :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.trace"},{"location":"api/utils/","text":"panacea.utils This page documents selected public utilities in panacea.utils that are commonly used in calibration and extraction workflows. .. automodule:: panacea.utils :members: safe_division, build_weight_matrix, robust_polyfit, read_arc_lines, get_config_file, get_bigarray, find_lines, create_header_objection :undoc-members: :show-inheritance: Examples - Safe division guarding against zeros/NaNs: import numpy as np from panacea.utils import safe_division num = np.array([1.0, 2.0, -3.0, 4.0]) denom = np.array([1.0, 0.0, np.inf, 1e-12]) out = safe_division(num, denom, eps=1e-8, fillval=0.0) print(out) # array([1.0, 0.0, 0.0, 0.0]) Build a spatial weight matrix between fibers: import numpy as np from panacea.utils import build_weight_matrix x = np.array([0.0, 1.0, 0.0, 1.0]) y = np.array([0.0, 0.0, 1.0, 1.0]) W = build_weight_matrix(x, y, sig=0.75) # columns sum to 1.0 assert np.allclose(W.sum(axis=0), 1.0)","title":"Utilities"},{"location":"api/utils/#panaceautils","text":"This page documents selected public utilities in panacea.utils that are commonly used in calibration and extraction workflows. .. automodule:: panacea.utils :members: safe_division, build_weight_matrix, robust_polyfit, read_arc_lines, get_config_file, get_bigarray, find_lines, create_header_objection :undoc-members: :show-inheritance: Examples - Safe division guarding against zeros/NaNs: import numpy as np from panacea.utils import safe_division num = np.array([1.0, 2.0, -3.0, 4.0]) denom = np.array([1.0, 0.0, np.inf, 1e-12]) out = safe_division(num, denom, eps=1e-8, fillval=0.0) print(out) # array([1.0, 0.0, 0.0, 0.0]) Build a spatial weight matrix between fibers: import numpy as np from panacea.utils import build_weight_matrix x = np.array([0.0, 1.0, 0.0, 1.0]) y = np.array([0.0, 0.0, 1.0, 1.0]) W = build_weight_matrix(x, y, sig=0.75) # columns sum to 1.0 assert np.allclose(W.sum(axis=0), 1.0)","title":"panacea.utils"},{"location":"api/wavelength/","text":"panacea.wavelength .. automodule:: panacea.wavelength :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.wavelength"},{"location":"api/wavelength/#panaceawavelength","text":".. automodule:: panacea.wavelength :members: :undoc-members: :show-inheritance: :inherited-members:","title":"panacea.wavelength"},{"location":"citation/citation/","text":"Citation If you use Panacea in your research, please cite it. The citation metadata is provided below (mirrored from the repository root CITATION.cff) so it is available directly within the docs site. cff-version: 1.2.0 message: If you use this software, please cite it using the metadata below. title: \"Panacea: LRS2 data-reduction pipeline for the Hobby-Eberly Telescope\" version: 1.0.1 date-released: 2026-01-14 license: BSD-3-Clause doi: 10.5281/zenodo.18250411 authors: - family-names: Zeimann given-names: Greg affiliation: University of Texas at Austin email: gregz@astro.as.utexas.edu orcid: https://orcid.org/0000-0003-2307-0629 repository-code: https://github.com/grzeimann/Panacea keywords: - astronomy - spectroscopy - data reduction - LRS2 preferred-citation: type: software title: Panacea authors: - family-names: Zeimann given-names: Greg year: 2026 doi: 10.5281/zenodo.18250411 notes: Versioned release (v1.0.1), but the doi above is for all versions. Notes - The canonical file still lives at the repository root: CITATION.cff . Keep the two in sync. - DOIs have been added: concept DOI for the project and version DOI for v1.0.1. See also: Changelog , Contributing","title":"Citation"},{"location":"citation/citation/#citation","text":"If you use Panacea in your research, please cite it. The citation metadata is provided below (mirrored from the repository root CITATION.cff) so it is available directly within the docs site. cff-version: 1.2.0 message: If you use this software, please cite it using the metadata below. title: \"Panacea: LRS2 data-reduction pipeline for the Hobby-Eberly Telescope\" version: 1.0.1 date-released: 2026-01-14 license: BSD-3-Clause doi: 10.5281/zenodo.18250411 authors: - family-names: Zeimann given-names: Greg affiliation: University of Texas at Austin email: gregz@astro.as.utexas.edu orcid: https://orcid.org/0000-0003-2307-0629 repository-code: https://github.com/grzeimann/Panacea keywords: - astronomy - spectroscopy - data reduction - LRS2 preferred-citation: type: software title: Panacea authors: - family-names: Zeimann given-names: Greg year: 2026 doi: 10.5281/zenodo.18250411 notes: Versioned release (v1.0.1), but the doi above is for all versions. Notes - The canonical file still lives at the repository root: CITATION.cff . Keep the two in sync. - DOIs have been added: concept DOI for the project and version DOI for v1.0.1. See also: Changelog , Contributing","title":"Citation"},{"location":"community/code_of_conduct/","text":"Code of Conduct We follow the Contributor Covenant Code of Conduct, version 2.1. Be respectful and inclusive. Communicate constructively and with empathy. No harassment, trolling, or discrimination. Respect differing viewpoints and experiences. Enforcement - Report unacceptable behavior to the maintainers at: gregz@astro.as.utexas.edu - Maintainers will review and respond promptly; actions may include warnings, temporary bans, or permanent exclusion. Attribution - This Code of Conduct is adapted from the Contributor Covenant, version 2.1 https://www.contributor-covenant.org/version/2/1/code_of_conduct.html See also: Contributing , Changelog","title":"Code of Conduct"},{"location":"community/code_of_conduct/#code-of-conduct","text":"We follow the Contributor Covenant Code of Conduct, version 2.1. Be respectful and inclusive. Communicate constructively and with empathy. No harassment, trolling, or discrimination. Respect differing viewpoints and experiences. Enforcement - Report unacceptable behavior to the maintainers at: gregz@astro.as.utexas.edu - Maintainers will review and respond promptly; actions may include warnings, temporary bans, or permanent exclusion. Attribution - This Code of Conduct is adapted from the Contributor Covenant, version 2.1 https://www.contributor-covenant.org/version/2/1/code_of_conduct.html See also: Contributing , Changelog","title":"Code of Conduct"},{"location":"community/contributing/","text":"Contributing to Panacea Thank you for your interest in contributing to Panacea! This document explains how to set up a development environment, coding standards, testing, and our pull request process. Since this is a Hobby-Eberly Telescope reduction pipeline, we encourage users of the telescope to collaborate on new developments. Getting started Fork the repository and create a feature branch from main . Ensure you are using Python 3.9+. Install development dependencies: python -m pip install -U pip python -m pip install -e .[dev] Running tests We use pytest. Once tests are added, run: pytest -q To see coverage (if configured): pytest --cov=panacea --cov-report=term-missing Building the docs locally (MkDocs) We keep documentation in the docs/ folder and use MkDocs for optional local previews. Prerequisites: - Python environment (same as development) - MkDocs Install MkDocs: pip install mkdocs Serve the docs locally from the repo root: mkdocs serve Then open the URL printed in the console (typically http://127.0.0.1:8000/) to preview the docs site. The site uses the Markdown files under docs/ directly; no extra configuration is required beyond mkdocs.yml . Coding style and docstrings Follow PEP8 Public functions/classes/methods should include Google\u2011style docstrings. Example: def example(a, b): \"\"\"Add two integers. Args: a: First number. b: Second number. Returns: The sum of a and b. Raises: ValueError: If inputs are invalid. \"\"\" return a + b Commit messages Use descriptive language. Reference issues if applicable. Pull requests Ensure tests pass locally: pytest . Update documentation. Open a PR against main with a clear description of the change and motivation. Release process (summary) Use semantic versioning. Tag releases as vMAJOR.MINOR.PATCH . After creating a release, update CITATION.cff and README badges. Code of Conduct By participating, you agree to abide by our Code of Conduct ( Code of Conduct ). See also: Changelog , Citation","title":"Contributing"},{"location":"community/contributing/#contributing-to-panacea","text":"Thank you for your interest in contributing to Panacea! This document explains how to set up a development environment, coding standards, testing, and our pull request process. Since this is a Hobby-Eberly Telescope reduction pipeline, we encourage users of the telescope to collaborate on new developments.","title":"Contributing to Panacea"},{"location":"community/contributing/#getting-started","text":"Fork the repository and create a feature branch from main . Ensure you are using Python 3.9+. Install development dependencies: python -m pip install -U pip python -m pip install -e .[dev]","title":"Getting started"},{"location":"community/contributing/#running-tests","text":"We use pytest. Once tests are added, run: pytest -q To see coverage (if configured): pytest --cov=panacea --cov-report=term-missing","title":"Running tests"},{"location":"community/contributing/#building-the-docs-locally-mkdocs","text":"We keep documentation in the docs/ folder and use MkDocs for optional local previews. Prerequisites: - Python environment (same as development) - MkDocs Install MkDocs: pip install mkdocs Serve the docs locally from the repo root: mkdocs serve Then open the URL printed in the console (typically http://127.0.0.1:8000/) to preview the docs site. The site uses the Markdown files under docs/ directly; no extra configuration is required beyond mkdocs.yml .","title":"Building the docs locally (MkDocs)"},{"location":"community/contributing/#coding-style-and-docstrings","text":"Follow PEP8 Public functions/classes/methods should include Google\u2011style docstrings. Example: def example(a, b): \"\"\"Add two integers. Args: a: First number. b: Second number. Returns: The sum of a and b. Raises: ValueError: If inputs are invalid. \"\"\" return a + b","title":"Coding style and docstrings"},{"location":"community/contributing/#commit-messages","text":"Use descriptive language. Reference issues if applicable.","title":"Commit messages"},{"location":"community/contributing/#pull-requests","text":"Ensure tests pass locally: pytest . Update documentation. Open a PR against main with a clear description of the change and motivation.","title":"Pull requests"},{"location":"community/contributing/#release-process-summary","text":"Use semantic versioning. Tag releases as vMAJOR.MINOR.PATCH . After creating a release, update CITATION.cff and README badges.","title":"Release process (summary)"},{"location":"community/contributing/#code-of-conduct","text":"By participating, you agree to abide by our Code of Conduct ( Code of Conduct ). See also: Changelog , Citation","title":"Code of Conduct"},{"location":"data-products/overview/","text":"Data Products This page summarizes the main outputs produced by Panacea and their contents. Primary products spectrum*.fits \u2014 produced for all exposures and channels. row1: wavelength (air) row2: extracted object spectrum (f_lambda: ergs/s/cm^2/A) row3: extracted sky spectrum from same aperture and weighting as object (s_lambda: ergs/s/cm^2/A) row4: error for extracted object spectrum (e_f_lambda: ergs/s/cm^2/A) row5: error for extracted sky spectrum (e_s_lambda: ergs/s/cm^2/A) row6: response function (ergs / e-) multi*{uv,orange,red,farred}.fits \u2014 multi-extension FITS containing: Rectified Spectra: flux calibrated spectrum (object + sky) for each fiber Rectified Sky Model:flux calibrated sky spectrum for each fiber Rectified Sky Subtracted Spectra: flux calibrated sky subtracted spectrum for each fiber Rectified Error Frame: flux calibrated error spectrum for each fiber Collapsed image: a collapsed frame for visualization of the source(s) Positions (IFU, Focal, Sky): ifu x and y positions, focal x and y position, and ra and dec Extracted Spectra and Response: This is identical to the spectrum*.fits extension above ADR: The atmospheric differential refraction as a function of wavelength. The columns are wavelength, x_adr, y_adr CCD Wavelength: The wavelength of each pixel in the 2d frame Image: the initial reduction of the 2d raw frame. Flat Fielded image: same as the image frame above but divided by the flat field (fiber profile and fiber to fiber normalization) Central Trace Pixels: location of the pixels for each fiber (central two pixels) Cosmics: identified cosmics in the central four pixels of the trace Unrectified Spectra: Unrectified, uncalibrated spectra for each fiber cube .fits \u2014 per-channel spectral datacubes. Spatial axes: IFU plane coordinates covering roughly 7\u00d711 arcsec per LRS2 IFU (one cube per channel). The pixel scale is set by the reconstruction grid. Spectral axis: wavelength in vacuum Angstroms (\u00c5) as the third dimension. Best use: quickly identify your target and strong emission features for quality control and visualization. These cubes are meant for inspection rather than precise spectrophotometry or cross\u2011channel combination. See also: TACC Overview , Algorithms","title":"Data Products"},{"location":"data-products/overview/#data-products","text":"This page summarizes the main outputs produced by Panacea and their contents.","title":"Data Products"},{"location":"data-products/overview/#primary-products","text":"spectrum*.fits \u2014 produced for all exposures and channels. row1: wavelength (air) row2: extracted object spectrum (f_lambda: ergs/s/cm^2/A) row3: extracted sky spectrum from same aperture and weighting as object (s_lambda: ergs/s/cm^2/A) row4: error for extracted object spectrum (e_f_lambda: ergs/s/cm^2/A) row5: error for extracted sky spectrum (e_s_lambda: ergs/s/cm^2/A) row6: response function (ergs / e-) multi*{uv,orange,red,farred}.fits \u2014 multi-extension FITS containing: Rectified Spectra: flux calibrated spectrum (object + sky) for each fiber Rectified Sky Model:flux calibrated sky spectrum for each fiber Rectified Sky Subtracted Spectra: flux calibrated sky subtracted spectrum for each fiber Rectified Error Frame: flux calibrated error spectrum for each fiber Collapsed image: a collapsed frame for visualization of the source(s) Positions (IFU, Focal, Sky): ifu x and y positions, focal x and y position, and ra and dec Extracted Spectra and Response: This is identical to the spectrum*.fits extension above ADR: The atmospheric differential refraction as a function of wavelength. The columns are wavelength, x_adr, y_adr CCD Wavelength: The wavelength of each pixel in the 2d frame Image: the initial reduction of the 2d raw frame. Flat Fielded image: same as the image frame above but divided by the flat field (fiber profile and fiber to fiber normalization) Central Trace Pixels: location of the pixels for each fiber (central two pixels) Cosmics: identified cosmics in the central four pixels of the trace Unrectified Spectra: Unrectified, uncalibrated spectra for each fiber cube .fits \u2014 per-channel spectral datacubes. Spatial axes: IFU plane coordinates covering roughly 7\u00d711 arcsec per LRS2 IFU (one cube per channel). The pixel scale is set by the reconstruction grid. Spectral axis: wavelength in vacuum Angstroms (\u00c5) as the third dimension. Best use: quickly identify your target and strong emission features for quality control and visualization. These cubes are meant for inspection rather than precise spectrophotometry or cross\u2011channel combination. See also: TACC Overview , Algorithms","title":"Primary products"},{"location":"development/dev-quickstart/","text":"Developer Quickstart (local CI, hooks, and reproducible env) This page collects the copy\u2011paste commands for creating a clean development environment, running the same checks as CI, and setting up pre\u2011commit hooks. If you already have a panacea conda environment conda deactivate conda env remove -n panacea Get the package git clone https://github.com/grzeimann/Panacea.git cd Panacea Create the environment and activate it conda env create -f environment.yml conda activate panacea Install in developer mode pip install .[dev] Initial run tests (sanity checks) panacea-lrs2 -h panacea-lrs2 --smoke-test If you are going to make code changes pre-commit clean pre-commit install --hook-type pre-push make ci pre-commit run --all-files # Stage and commit tracked changes git add -u git commit -m \"message\" # If pre-commit hooks modify files on commit (EOF/trailing whitespace, ruff --fix), re\u2011stage and commit those auto\u2011fixes git add -u && git commit -m \"Apply pre-commit auto-fixes\" # Push to your remote git push Notes - The pre\u2011push hook runs tests under coverage in an isolated environment pinned to compatible versions (numpy<2.0 with astropy<6). If it\u2019s too slow for your workflow, you can skip once with git push --no-verify (not recommended routinely). - Makefile targets mirror CI: - make lint \u2192 ruff check - make test \u2192 pytest -q - make coverage \u2192 coverage run/report/xml - make ci \u2192 lint + coverage (closest to CI) Verify docs build (Sphinx) If you recently hit a Sphinx error like \"Linkify enabled but not installed\", we fixed this by adding linkify-it-py to the docs extras and enabling MyST linkify only when available. Verify locally with: # Ensure docs extras (includes linkify-it-py and optional themes) are installed pip install -U .[docs] # Build the HTML docs sphinx-build -b html docs docs/_build/html # Open the result (macOS example) open docs/_build/html/index.html # use xdg-open on Linux Switching Sphinx themes (quick ways) Option A: One-off build with an environment variable # Use Furo SPHINX_THEME=furo sphinx-build -b html docs docs/_build/html # Use PyData SPHINX_THEME=pydata_sphinx_theme sphinx-build -b html docs docs/_build/html # Use Book theme SPHINX_THEME=sphinx_book_theme sphinx-build -b html docs docs/_build/html # Use Read the Docs theme SPHINX_THEME=sphinx_rtd_theme sphinx-build -b html docs docs/_build/html Option B: Make it the default (edit docs/conf.py) - Set html_theme = \"pydata_sphinx_theme\" (or another) near the bottom where theme selection occurs, or export SPHINX_THEME in your shell/profile. - The config auto-detects installed themes and falls back gracefully. Linkify backend status (optional check) - During the build, you should see a log line like: - [Panacea docs] MyST linkify backend: ENABLED or DISABLED - The generated HTML also includes a meta tag recording the status. View page source and look for: - or \"disabled\" - If DISABLED, this is okay: docs still build and render; only auto-linking of bare URLs is skipped. To enable, install docs extras: pip install .[docs] . Stage, commit, push (exact commands) These commands are safe to copy\u2011paste. They cover the common cases and a few handy fixes. Staging and committing tracked changes only # Stage modifications and deletions to already-tracked files git add -u # Optional: review what will be committed git status -s git diff --staged # Commit git commit -m \"Describe your change\" Including new, untracked files (additions) # Stage everything, including new files and deletions git add -A # Optional: review git status -s git diff --staged # Commit git commit -m \"Describe your change\" Pushing your branch # Push current branch to origin git push # If this is a new local branch tracking a remote for the first time git push -u origin $(git branch --show-current) If pre-commit modifies files during commit # When hooks auto-fix (EOF newline, trailing whitespace, ruff --fix), re-stage and commit the fixes git add -u && git commit -m \"Apply pre-commit auto-fixes\" Helpful one-offs # See unstaged vs staged changes git status -s git diff # unstaged git diff --staged # staged # Unstage everything (keep working tree changes) git restore --staged . # Amend the last commit message or include newly-staged changes git commit --amend --no-edit # keep the same message # Bypass hooks once (not recommended routinely) git push --no-verify # Clean ignored build artifacts (safe preview first) git clean -ndX # preview removal of ignored files git clean -fdX # actually remove ignored files Tip: If your push is blocked by the pre-push test hook and you\u2019ve already run the full local CI ( make ci ), you can skip once with git push --no-verify . Prefer fixing failures so that hooks remain helpful.","title":"Developer Quickstart (local CI, hooks, and reproducible env)"},{"location":"development/dev-quickstart/#developer-quickstart-local-ci-hooks-and-reproducible-env","text":"This page collects the copy\u2011paste commands for creating a clean development environment, running the same checks as CI, and setting up pre\u2011commit hooks. If you already have a panacea conda environment conda deactivate conda env remove -n panacea Get the package git clone https://github.com/grzeimann/Panacea.git cd Panacea Create the environment and activate it conda env create -f environment.yml conda activate panacea Install in developer mode pip install .[dev] Initial run tests (sanity checks) panacea-lrs2 -h panacea-lrs2 --smoke-test If you are going to make code changes pre-commit clean pre-commit install --hook-type pre-push make ci pre-commit run --all-files # Stage and commit tracked changes git add -u git commit -m \"message\" # If pre-commit hooks modify files on commit (EOF/trailing whitespace, ruff --fix), re\u2011stage and commit those auto\u2011fixes git add -u && git commit -m \"Apply pre-commit auto-fixes\" # Push to your remote git push Notes - The pre\u2011push hook runs tests under coverage in an isolated environment pinned to compatible versions (numpy<2.0 with astropy<6). If it\u2019s too slow for your workflow, you can skip once with git push --no-verify (not recommended routinely). - Makefile targets mirror CI: - make lint \u2192 ruff check - make test \u2192 pytest -q - make coverage \u2192 coverage run/report/xml - make ci \u2192 lint + coverage (closest to CI) Verify docs build (Sphinx) If you recently hit a Sphinx error like \"Linkify enabled but not installed\", we fixed this by adding linkify-it-py to the docs extras and enabling MyST linkify only when available. Verify locally with: # Ensure docs extras (includes linkify-it-py and optional themes) are installed pip install -U .[docs] # Build the HTML docs sphinx-build -b html docs docs/_build/html # Open the result (macOS example) open docs/_build/html/index.html # use xdg-open on Linux Switching Sphinx themes (quick ways) Option A: One-off build with an environment variable # Use Furo SPHINX_THEME=furo sphinx-build -b html docs docs/_build/html # Use PyData SPHINX_THEME=pydata_sphinx_theme sphinx-build -b html docs docs/_build/html # Use Book theme SPHINX_THEME=sphinx_book_theme sphinx-build -b html docs docs/_build/html # Use Read the Docs theme SPHINX_THEME=sphinx_rtd_theme sphinx-build -b html docs docs/_build/html Option B: Make it the default (edit docs/conf.py) - Set html_theme = \"pydata_sphinx_theme\" (or another) near the bottom where theme selection occurs, or export SPHINX_THEME in your shell/profile. - The config auto-detects installed themes and falls back gracefully. Linkify backend status (optional check) - During the build, you should see a log line like: - [Panacea docs] MyST linkify backend: ENABLED or DISABLED - The generated HTML also includes a meta tag recording the status. View page source and look for: - or \"disabled\" - If DISABLED, this is okay: docs still build and render; only auto-linking of bare URLs is skipped. To enable, install docs extras: pip install .[docs] .","title":"Developer Quickstart (local CI, hooks, and reproducible env)"},{"location":"development/dev-quickstart/#stage-commit-push-exact-commands","text":"These commands are safe to copy\u2011paste. They cover the common cases and a few handy fixes. Staging and committing tracked changes only # Stage modifications and deletions to already-tracked files git add -u # Optional: review what will be committed git status -s git diff --staged # Commit git commit -m \"Describe your change\" Including new, untracked files (additions) # Stage everything, including new files and deletions git add -A # Optional: review git status -s git diff --staged # Commit git commit -m \"Describe your change\" Pushing your branch # Push current branch to origin git push # If this is a new local branch tracking a remote for the first time git push -u origin $(git branch --show-current) If pre-commit modifies files during commit # When hooks auto-fix (EOF newline, trailing whitespace, ruff --fix), re-stage and commit the fixes git add -u && git commit -m \"Apply pre-commit auto-fixes\" Helpful one-offs # See unstaged vs staged changes git status -s git diff # unstaged git diff --staged # staged # Unstage everything (keep working tree changes) git restore --staged . # Amend the last commit message or include newly-staged changes git commit --amend --no-edit # keep the same message # Bypass hooks once (not recommended routinely) git push --no-verify # Clean ignored build artifacts (safe preview first) git clean -ndX # preview removal of ignored files git clean -fdX # actually remove ignored files Tip: If your push is blocked by the pre-push test hook and you\u2019ve already run the full local CI ( make ci ), you can skip once with git push --no-verify . Prefer fixing failures so that hooks remain helpful.","title":"Stage, commit, push (exact commands)"},{"location":"development/tests-overview/","text":"Tests overview (what we cover and where) This page explains where the project\u2019s tests live and what the most important ones validate. It\u2019s intended to help users, reviewers (e.g., JOSS), and new contributors understand how to run and interpret the suite. Where tests live - All tests are under the repository\u2019s tests/ directory. - They are designed to be fast and data\u2011free except where noted; you can run them locally with: - make test (or) pytest -q - For coverage: coverage run -m pytest -q && coverage report -m Key tests and what they check - CLI and wiring - tests/test_cli.py \u2014 console entry panacea-lrs2 exists; -h exits 0 - tests/test_cli_smoke.py \u2014 simulates panacea-lrs2 -h and checks help text - tests/test_pipeline_smoke.py \u2014 uses the hidden --smoke-test path to verify that packaged resources are discoverable for all default sides without touching raw data - Parsers and resources - tests/test_parsers.py \u2014 validates simple parsing against packaged config files (DAR tables, skylines) and the read_arc_lines helper returns an Astropy Table with expected columns - tests/test_resources.py \u2014 ensures packaged resource files are found via importlib.resources and that read_arc_lines parses minimal valid content - Calibration and utilities - tests/test_calibration_utils.py \u2014 unit tests for safe_division, build_weight_matrix, robust_polyfit, and response\u2011continuum estimation - Sample data sanity (for reviewers) - tests/test_sample_data.py \u2014 uses a tiny synthetic dataset created on the fly (via tests/fixtures/) to assert that expected FITS files (bias, flat, arc, twilight, science) exist with small shapes and basic headers (INSTRUME=LRS2, CHANNEL present, EXPTIME set). It also checks that the arc has columns significantly brighter than median (a simple spectral\u2011line heuristic). How this relates to the sample dataset (~400 MB) - For a realistic end\u2011to\u2011end run, see Quickstart \u2192 \u201cRun on sample data (~400 MB)\u201d. Those instructions are meant for users or reviewers who want to try a real reduction quickly. - The tests/test_sample_data.py file is intentionally small and synthetic to keep CI fast; it is not the same as the ~400 MB dataset used in the Quickstart run. The two complement each other: - tests/test_sample_data.py proves readers and I/O paths work on representative FITS structures - The Quickstart sample dataset demonstrates full pipeline behavior on real data How to run only selected tests - Single file: pytest tests/test_parsers.py -q - Single test by name: pytest -k test_read_arc_lines_stringio_basic -q - Stop on first failure (handy when debugging): pytest -x -q See also - Developer quickstart (env, hooks, CI parity): ../development/dev-quickstart.md - Quickstart sample dataset and run: ../getting-started/quickstart.md","title":"Tests overview (what we cover and where)"},{"location":"development/tests-overview/#tests-overview-what-we-cover-and-where","text":"This page explains where the project\u2019s tests live and what the most important ones validate. It\u2019s intended to help users, reviewers (e.g., JOSS), and new contributors understand how to run and interpret the suite. Where tests live - All tests are under the repository\u2019s tests/ directory. - They are designed to be fast and data\u2011free except where noted; you can run them locally with: - make test (or) pytest -q - For coverage: coverage run -m pytest -q && coverage report -m Key tests and what they check - CLI and wiring - tests/test_cli.py \u2014 console entry panacea-lrs2 exists; -h exits 0 - tests/test_cli_smoke.py \u2014 simulates panacea-lrs2 -h and checks help text - tests/test_pipeline_smoke.py \u2014 uses the hidden --smoke-test path to verify that packaged resources are discoverable for all default sides without touching raw data - Parsers and resources - tests/test_parsers.py \u2014 validates simple parsing against packaged config files (DAR tables, skylines) and the read_arc_lines helper returns an Astropy Table with expected columns - tests/test_resources.py \u2014 ensures packaged resource files are found via importlib.resources and that read_arc_lines parses minimal valid content - Calibration and utilities - tests/test_calibration_utils.py \u2014 unit tests for safe_division, build_weight_matrix, robust_polyfit, and response\u2011continuum estimation - Sample data sanity (for reviewers) - tests/test_sample_data.py \u2014 uses a tiny synthetic dataset created on the fly (via tests/fixtures/) to assert that expected FITS files (bias, flat, arc, twilight, science) exist with small shapes and basic headers (INSTRUME=LRS2, CHANNEL present, EXPTIME set). It also checks that the arc has columns significantly brighter than median (a simple spectral\u2011line heuristic). How this relates to the sample dataset (~400 MB) - For a realistic end\u2011to\u2011end run, see Quickstart \u2192 \u201cRun on sample data (~400 MB)\u201d. Those instructions are meant for users or reviewers who want to try a real reduction quickly. - The tests/test_sample_data.py file is intentionally small and synthetic to keep CI fast; it is not the same as the ~400 MB dataset used in the Quickstart run. The two complement each other: - tests/test_sample_data.py proves readers and I/O paths work on representative FITS structures - The Quickstart sample dataset demonstrates full pipeline behavior on real data How to run only selected tests - Single file: pytest tests/test_parsers.py -q - Single test by name: pytest -k test_read_arc_lines_stringio_basic -q - Stop on first failure (handy when debugging): pytest -x -q See also - Developer quickstart (env, hooks, CI parity): ../development/dev-quickstart.md - Quickstart sample dataset and run: ../getting-started/quickstart.md","title":"Tests overview (what we cover and where)"},{"location":"getting-started/installation/","text":"Installation This page covers local installation and dependencies. For TACC usage, see ../tacc/overview.md. Dependencies Python 3.9+ NumPy, SciPy, Astropy, Matplotlib, PyYAML, tqdm, requests, scikit-learn (installed automatically from PyPI) Create a conda environment and install conda env create -f environment.yml # run from the repository root conda activate panacea # Install Panacea (use --no-deps because conda provided most packages) pip install . --no-deps Notes: - Some steps require significant memory/storage; for full-scale reductions consider running on TACC (see ../tacc/overview.md). See also: Quickstart , TACC Overview","title":"Installation"},{"location":"getting-started/installation/#installation","text":"This page covers local installation and dependencies. For TACC usage, see ../tacc/overview.md.","title":"Installation"},{"location":"getting-started/installation/#dependencies","text":"Python 3.9+ NumPy, SciPy, Astropy, Matplotlib, PyYAML, tqdm, requests, scikit-learn (installed automatically from PyPI)","title":"Dependencies"},{"location":"getting-started/installation/#create-a-conda-environment-and-install","text":"conda env create -f environment.yml # run from the repository root conda activate panacea # Install Panacea (use --no-deps because conda provided most packages) pip install . --no-deps Notes: - Some steps require significant memory/storage; for full-scale reductions consider running on TACC (see ../tacc/overview.md). See also: Quickstart , TACC Overview","title":"Create a conda environment and install"},{"location":"getting-started/quickstart/","text":"Quickstart This page shows first commands to verify your installation and run basic reductions locally. For running on TACC, see ../tacc/running.md. Verify installation panacea-lrs2 -h Example commands Adjust arguments to your data and environment. # Reduce Far Red for a specific date panacea-lrs2 -d 20260115 -s farred # Reduce UV for a specific date panacea-lrs2 -d 20260115 -s uv Run on sample data (~400 MB) If new users need to try a real reduction quickly, a small sample dataset is available. Replace DATA_FOLDER and OUTPUT_FOLDER with your paths. # 1) Download the sample tarball (about 400 MB) # Browse to: https://web.corral.tacc.utexas.edu/hetdex/LRS2_test_data/ # Download file: lrs2_20260115_test.tar.gz # 2) Move it into your chosen data directory mv lrs2_20260115_test.tar.gz DATA_FOLDER/. # 3) Extract (and remove the tarball to save space) cd DATA_FOLDER tar -xvzf lrs2_20260115_test.tar.gz && rm lrs2_20260115_test.tar.gz # 4) Run the pipeline from your desired output location (or any directory in the same environment) cd OUTPUT_FOLDER # Example: reduce the Orange channel for this date using your data folder as --baseraw panacea-lrs2 -d 20260115 --baseraw DATA_FOLDER -s orange What to expect - Output directory LRS2/PROGRAM-ID will be created under your current working directory. - For this sample, PROGRAM-ID values will be CALS and ORPHANS. - See Data products overview for details on files and structure. Notes for local runs The quicklook runner expects HET raw data in a TACC-like directory structure (tarballs with standard internal paths). Use the --baseraw option to point Panacea to your local mirror of the LRS2 raw data. Example: --baseraw /data/LRS2 Packaged configuration files (line lists, DAR tables, fplane.txt, responses) are bundled with the package and found automatically via importlib.resources. See also: CLI Usage , Configuration , and Tests overview","title":"Quickstart"},{"location":"getting-started/quickstart/#quickstart","text":"This page shows first commands to verify your installation and run basic reductions locally. For running on TACC, see ../tacc/running.md.","title":"Quickstart"},{"location":"getting-started/quickstart/#verify-installation","text":"panacea-lrs2 -h","title":"Verify installation"},{"location":"getting-started/quickstart/#example-commands","text":"Adjust arguments to your data and environment. # Reduce Far Red for a specific date panacea-lrs2 -d 20260115 -s farred # Reduce UV for a specific date panacea-lrs2 -d 20260115 -s uv","title":"Example commands"},{"location":"getting-started/quickstart/#run-on-sample-data-400-mb","text":"If new users need to try a real reduction quickly, a small sample dataset is available. Replace DATA_FOLDER and OUTPUT_FOLDER with your paths. # 1) Download the sample tarball (about 400 MB) # Browse to: https://web.corral.tacc.utexas.edu/hetdex/LRS2_test_data/ # Download file: lrs2_20260115_test.tar.gz # 2) Move it into your chosen data directory mv lrs2_20260115_test.tar.gz DATA_FOLDER/. # 3) Extract (and remove the tarball to save space) cd DATA_FOLDER tar -xvzf lrs2_20260115_test.tar.gz && rm lrs2_20260115_test.tar.gz # 4) Run the pipeline from your desired output location (or any directory in the same environment) cd OUTPUT_FOLDER # Example: reduce the Orange channel for this date using your data folder as --baseraw panacea-lrs2 -d 20260115 --baseraw DATA_FOLDER -s orange What to expect - Output directory LRS2/PROGRAM-ID will be created under your current working directory. - For this sample, PROGRAM-ID values will be CALS and ORPHANS. - See Data products overview for details on files and structure.","title":"Run on sample data (~400 MB)"},{"location":"getting-started/quickstart/#notes-for-local-runs","text":"The quicklook runner expects HET raw data in a TACC-like directory structure (tarballs with standard internal paths). Use the --baseraw option to point Panacea to your local mirror of the LRS2 raw data. Example: --baseraw /data/LRS2 Packaged configuration files (line lists, DAR tables, fplane.txt, responses) are bundled with the package and found automatically via importlib.resources. See also: CLI Usage , Configuration , and Tests overview","title":"Notes for local runs"},{"location":"migration/sphinx/","text":"Using Sphinx with Markdown (MyST) This repo\u2019s documentation is written in plain Markdown with relative links (./ and ../). It renders correctly on GitHub and with MkDocs. If you prefer Sphinx, you can keep the same Markdown files and relative links by enabling the MyST parser. Why relative links are fine GitHub: relative links are resolved based on the current file\u2019s path. MkDocs: relative links are resolved against the docs/ directory and work as expected. Sphinx + MyST: relative Markdown links between project files are supported and will resolve as document references, provided the linked files are included in Sphinx\u2019s source tree (e.g., under docs/). For maximum robustness in Sphinx, you may also use Sphinx roles like {doc} and section labels, but this is optional if your links are simple ./file.md or ../path/file.md references. Minimal Sphinx setup (keeping Markdown) 1) Install Sphinx and MyST pip install sphinx myst-parser # Optional: if you enable MyST's 'linkify' feature, install its backend: pip install linkify-it-py # Optional: to get a persistent sidebar/navigation like the docs site, install the RTD theme: pip install sphinx_rtd_theme 2) Create docs/conf.py (example) # docs/conf.py project = 'Panacea Documentation' extensions = [ 'myst_parser', ] # Treat .md files as source source_suffix = { '.md': 'markdown', '.rst': 'restructuredtext', } # Root document (our docs/ already has index.md) master_doc = 'index' # Optional: add anchors to headings so #section links work myst_heading_anchors = 3 # Optional: enable extra MyST features if desired myst_enable_extensions = [ 'linkify', # autolink URLs in text ] # If using relative image paths, Sphinx resolves them like Markdown html_theme = 'alabaster' # or any installed Sphinx theme 3) Ensure your index - You already have docs/index.md . Sphinx will use it as the root ( master_doc = 'index' ). - Make sure all pages you want included are reachable from index.md via links, or list them in a toctree (see below). 4) (Optional) Add an index toctree if you want sidebar navigation Add this block somewhere in docs/index.md to drive Sphinx nav (MkDocs will ignore it). NOTE: This is an example shown as a literal code block so it does not affect your build from this page. {toctree} :hidden: :maxdepth: 2 getting-started/installation.md getting-started/quickstart.md tacc/overview.md tacc/running.md user-guide/cli.md user-guide/configuration.md data-products/overview.md algorithms/overview.md faq.md community/contributing.md community/code_of_conduct.md release_info/changelog.md citation/citation.md 5) Build sphinx-build -b html docs/ _build/html Open _build/html/index.html in your browser. Notes and caveats Relative links like ./tacc/overview.md or ../data-products/overview.md will resolve under MyST so long as the target files are in the Sphinx source ( docs/ ). For cross-page section anchors (e.g., overview.md#where-to-find-data-products ), set myst_heading_anchors (as above) so Sphinx creates stable IDs for headings. For advanced cross-references, prefer MyST/Sphinx roles, e.g., [{doc}](/path) \u2192 [TACC Overview]({doc} tacc/overview ) or labeled headings with {#my-label} and use {ref} roles. This increases resilience to file renames. If you later mix .rst and .md , both are supported with the source_suffix mapping shown above. Our existing MkDocs config ( mkdocs.yml ) continues to work independently; you can keep both builders side-by-side. Optional: API docs from source code Sphinx: Already configured. We enabled autodoc , autosummary , and napoleon in docs/conf.py and added docs/api/index.md . Build with sphinx-build -b html docs/ _build/html to generate API pages under the sidebar \"API Reference\". MkDocs: If you prefer API under MkDocs, install mkdocstrings[python] and add an API section to mkdocs.yml . Example snippet: plugins: - mkdocstrings: handlers: python: options: docstring_style: google nav: - API: - panacea: api/panacea.md Then create a page like docs/api/panacea.md with: # panacea ::: panacea See also: Docs Guide , Home","title":"Using Sphinx with Markdown (MyST)"},{"location":"migration/sphinx/#using-sphinx-with-markdown-myst","text":"This repo\u2019s documentation is written in plain Markdown with relative links (./ and ../). It renders correctly on GitHub and with MkDocs. If you prefer Sphinx, you can keep the same Markdown files and relative links by enabling the MyST parser.","title":"Using Sphinx with Markdown (MyST)"},{"location":"migration/sphinx/#why-relative-links-are-fine","text":"GitHub: relative links are resolved based on the current file\u2019s path. MkDocs: relative links are resolved against the docs/ directory and work as expected. Sphinx + MyST: relative Markdown links between project files are supported and will resolve as document references, provided the linked files are included in Sphinx\u2019s source tree (e.g., under docs/). For maximum robustness in Sphinx, you may also use Sphinx roles like {doc} and section labels, but this is optional if your links are simple ./file.md or ../path/file.md references.","title":"Why relative links are fine"},{"location":"migration/sphinx/#minimal-sphinx-setup-keeping-markdown","text":"1) Install Sphinx and MyST pip install sphinx myst-parser # Optional: if you enable MyST's 'linkify' feature, install its backend: pip install linkify-it-py # Optional: to get a persistent sidebar/navigation like the docs site, install the RTD theme: pip install sphinx_rtd_theme 2) Create docs/conf.py (example) # docs/conf.py project = 'Panacea Documentation' extensions = [ 'myst_parser', ] # Treat .md files as source source_suffix = { '.md': 'markdown', '.rst': 'restructuredtext', } # Root document (our docs/ already has index.md) master_doc = 'index' # Optional: add anchors to headings so #section links work myst_heading_anchors = 3 # Optional: enable extra MyST features if desired myst_enable_extensions = [ 'linkify', # autolink URLs in text ] # If using relative image paths, Sphinx resolves them like Markdown html_theme = 'alabaster' # or any installed Sphinx theme 3) Ensure your index - You already have docs/index.md . Sphinx will use it as the root ( master_doc = 'index' ). - Make sure all pages you want included are reachable from index.md via links, or list them in a toctree (see below). 4) (Optional) Add an index toctree if you want sidebar navigation Add this block somewhere in docs/index.md to drive Sphinx nav (MkDocs will ignore it). NOTE: This is an example shown as a literal code block so it does not affect your build from this page. {toctree} :hidden: :maxdepth: 2 getting-started/installation.md getting-started/quickstart.md tacc/overview.md tacc/running.md user-guide/cli.md user-guide/configuration.md data-products/overview.md algorithms/overview.md faq.md community/contributing.md community/code_of_conduct.md release_info/changelog.md citation/citation.md 5) Build sphinx-build -b html docs/ _build/html Open _build/html/index.html in your browser.","title":"Minimal Sphinx setup (keeping Markdown)"},{"location":"migration/sphinx/#notes-and-caveats","text":"Relative links like ./tacc/overview.md or ../data-products/overview.md will resolve under MyST so long as the target files are in the Sphinx source ( docs/ ). For cross-page section anchors (e.g., overview.md#where-to-find-data-products ), set myst_heading_anchors (as above) so Sphinx creates stable IDs for headings. For advanced cross-references, prefer MyST/Sphinx roles, e.g., [{doc}](/path) \u2192 [TACC Overview]({doc} tacc/overview ) or labeled headings with {#my-label} and use {ref} roles. This increases resilience to file renames. If you later mix .rst and .md , both are supported with the source_suffix mapping shown above. Our existing MkDocs config ( mkdocs.yml ) continues to work independently; you can keep both builders side-by-side.","title":"Notes and caveats"},{"location":"migration/sphinx/#optional-api-docs-from-source-code","text":"Sphinx: Already configured. We enabled autodoc , autosummary , and napoleon in docs/conf.py and added docs/api/index.md . Build with sphinx-build -b html docs/ _build/html to generate API pages under the sidebar \"API Reference\". MkDocs: If you prefer API under MkDocs, install mkdocstrings[python] and add an API section to mkdocs.yml . Example snippet: plugins: - mkdocstrings: handlers: python: options: docstring_style: google nav: - API: - panacea: api/panacea.md Then create a page like docs/api/panacea.md with: # panacea ::: panacea See also: Docs Guide , Home","title":"Optional: API docs from source code"},{"location":"release_info/changelog/","text":"Changelog How we write future entries - Heading format: [X.Y.Z] - YYYY-MM-DD - Start with a 2\u20135 line summary of why the release matters for users. - Use simple sections only when they help: Added, Changed, Fixed, Docs, Dev. - One bullet per item; link issues/PRs when relevant (e.g., #123). - Call out any breaking changes clearly. Example [unreleased] - Short bullets about things merged since the last tag. [1.0.1] - 2026-01-14 \u2014 Patch: Zenodo/JOSS metadata fixes This patch release updates project metadata used by Zenodo and the documentation site. No runtime code changes. Changed Bump package version to 1.0.1 in pyproject.toml. Update CITATION.cff version to 1.0.1 (date unchanged for this patch). Update Sphinx docs version banner (docs/conf.py release = \"1.0.1\"). Docs Changelog updated with this entry. [1.0.0] - 2026-01-14 \u2014 Initial release for Zenodo/JOSS This tag marks the first stable, citable Panacea release submitted to the Journal of Open Source Software (JOSS). It completes the refactor from a single monolithic script into a maintainable, modular Python package with basic tests, uniform documentation, and an open path for collaboration within the HET community. Highlights Refactor: split the legacy monolithic pipeline into cohesive modules (ccd, fiber, sky, wavelength, trace, routine, astrometry, utils, io) with clear responsibilities and imports. CLI: panacea-lrs2 wraps the quicklook reduction with concise, practical options for nightly use. Reproducibility: bundles required configuration resources (line lists, DAR tables, responses, fiber locations, fplane) inside the package for consistent results. Added Initial test coverage in tests/ to exercise parsers, resource loaders, and core calibration utilities. Packaged configuration data under panacea/lrs2_config loaded via importlib.resources. Docs Consolidated documentation site with user guides (installation, CLI, configuration, data products), API reference, and FAQ. More uniform, concise docstrings across modules; improved README with links to contributing and citation. [0.0.5] - 2019-01-01 \u2014 Start of automated processing (legacy CLI) This internal milestone marks the first automated nightly reductions using the original command-line driver, full_reduction.py \u2014 a single ~2000-line monolithic script that orchestrated the pipeline end to end. - Automatic processing began on 2019-01-01 using the legacy CLI. - Established end-to-end quicklook reductions with minimal configuration. See also: Citation , Contributing","title":"Changelog"},{"location":"release_info/changelog/#changelog","text":"How we write future entries - Heading format: [X.Y.Z] - YYYY-MM-DD - Start with a 2\u20135 line summary of why the release matters for users. - Use simple sections only when they help: Added, Changed, Fixed, Docs, Dev. - One bullet per item; link issues/PRs when relevant (e.g., #123). - Call out any breaking changes clearly. Example [unreleased] - Short bullets about things merged since the last tag.","title":"Changelog"},{"location":"release_info/changelog/#101-2026-01-14-patch-zenodojoss-metadata-fixes","text":"This patch release updates project metadata used by Zenodo and the documentation site. No runtime code changes.","title":"[1.0.1] - 2026-01-14 \u2014 Patch: Zenodo/JOSS metadata fixes"},{"location":"release_info/changelog/#changed","text":"Bump package version to 1.0.1 in pyproject.toml. Update CITATION.cff version to 1.0.1 (date unchanged for this patch). Update Sphinx docs version banner (docs/conf.py release = \"1.0.1\").","title":"Changed"},{"location":"release_info/changelog/#docs","text":"Changelog updated with this entry.","title":"Docs"},{"location":"release_info/changelog/#100-2026-01-14-initial-release-for-zenodojoss","text":"This tag marks the first stable, citable Panacea release submitted to the Journal of Open Source Software (JOSS). It completes the refactor from a single monolithic script into a maintainable, modular Python package with basic tests, uniform documentation, and an open path for collaboration within the HET community.","title":"[1.0.0] - 2026-01-14 \u2014 Initial release for Zenodo/JOSS"},{"location":"release_info/changelog/#highlights","text":"Refactor: split the legacy monolithic pipeline into cohesive modules (ccd, fiber, sky, wavelength, trace, routine, astrometry, utils, io) with clear responsibilities and imports. CLI: panacea-lrs2 wraps the quicklook reduction with concise, practical options for nightly use. Reproducibility: bundles required configuration resources (line lists, DAR tables, responses, fiber locations, fplane) inside the package for consistent results.","title":"Highlights"},{"location":"release_info/changelog/#added","text":"Initial test coverage in tests/ to exercise parsers, resource loaders, and core calibration utilities. Packaged configuration data under panacea/lrs2_config loaded via importlib.resources.","title":"Added"},{"location":"release_info/changelog/#docs_1","text":"Consolidated documentation site with user guides (installation, CLI, configuration, data products), API reference, and FAQ. More uniform, concise docstrings across modules; improved README with links to contributing and citation.","title":"Docs"},{"location":"release_info/changelog/#005-2019-01-01-start-of-automated-processing-legacy-cli","text":"This internal milestone marks the first automated nightly reductions using the original command-line driver, full_reduction.py \u2014 a single ~2000-line monolithic script that orchestrated the pipeline end to end. - Automatic processing began on 2019-01-01 using the legacy CLI. - Established end-to-end quicklook reductions with minimal configuration. See also: Citation , Contributing","title":"[0.0.5] - 2019-01-01 \u2014 Start of automated processing (legacy CLI)"},{"location":"tacc/overview/","text":"TACC Overview This page is for HET users running or accessing Panacea reductions on the Texas Advanced Computing Center (TACC). Accounts and Access Create an account: https://portal.tacc.utexas.edu/ After your account is created, email Greg Zeimann gregz@astro.as.utexas.edu your TACC username to be added to the HET group. SSH into TACC: ssh -Y USERNAME@ls6.tacc.utexas.edu Where to find data products Daily automated reductions are written to: /work/03946/hetdex/maverick/LRS2/PROGRAM-ID Replace PROGRAM-ID with your program number (e.g., HET19-1-999). To copy reductions for your program to your local machine: scp -r username@ls6.tacc.utexas.edu:/work/03946/hetdex/maverick/LRS2/PROGRAM-ID . To fetch specific files for a given date: scp username@ls6.tacc.utexas.edu:/work/03946/hetdex/maverick/LRS2/PROGRAM-ID/spec*20190105*.fits . See also: Running on TACC , Data Products","title":"Overview"},{"location":"tacc/overview/#tacc-overview","text":"This page is for HET users running or accessing Panacea reductions on the Texas Advanced Computing Center (TACC).","title":"TACC Overview"},{"location":"tacc/overview/#accounts-and-access","text":"Create an account: https://portal.tacc.utexas.edu/ After your account is created, email Greg Zeimann gregz@astro.as.utexas.edu your TACC username to be added to the HET group. SSH into TACC: ssh -Y USERNAME@ls6.tacc.utexas.edu","title":"Accounts and Access"},{"location":"tacc/overview/#where-to-find-data-products","text":"Daily automated reductions are written to: /work/03946/hetdex/maverick/LRS2/PROGRAM-ID Replace PROGRAM-ID with your program number (e.g., HET19-1-999). To copy reductions for your program to your local machine: scp -r username@ls6.tacc.utexas.edu:/work/03946/hetdex/maverick/LRS2/PROGRAM-ID . To fetch specific files for a given date: scp username@ls6.tacc.utexas.edu:/work/03946/hetdex/maverick/LRS2/PROGRAM-ID/spec*20190105*.fits . See also: Running on TACC , Data Products","title":"Where to find data products"},{"location":"tacc/running/","text":"Running on TACC This page describes interactive and batch ways to run Panacea on TACC. Interactive exploration Start a development session and inspect CLI help: idev panacea-lrs2 -h Common options: - -d, --date YYYYMMDD date of observations (required) - -s, --sides uv,orange,red,farred which channels to reduce (comma-separated) - -o, --object NAME match target OBJECT header containing NAME - --use_flat , --correct_ftf - --central_wave , --wavelength_bin , --source_x , --source_y - --standard_star_date , --standard_star_obsid Example: panacea-lrs2 -d 20181108 -s uv -o HD_19445 Batch submission To run reductions in batch for a specific target and date on all four channels: cdw cp /work/03946/hetdex/maverick/run_lrs2/runlrs2general . runlrs2general DATE TARGET_NAME You will receive a job number upon submission. Monitor logs such as reductionlrs2daily.oXXXXXX for progress. See also: TACC Overview , CLI Usage","title":"Running on TACC"},{"location":"tacc/running/#running-on-tacc","text":"This page describes interactive and batch ways to run Panacea on TACC.","title":"Running on TACC"},{"location":"tacc/running/#interactive-exploration","text":"Start a development session and inspect CLI help: idev panacea-lrs2 -h Common options: - -d, --date YYYYMMDD date of observations (required) - -s, --sides uv,orange,red,farred which channels to reduce (comma-separated) - -o, --object NAME match target OBJECT header containing NAME - --use_flat , --correct_ftf - --central_wave , --wavelength_bin , --source_x , --source_y - --standard_star_date , --standard_star_obsid Example: panacea-lrs2 -d 20181108 -s uv -o HD_19445","title":"Interactive exploration"},{"location":"tacc/running/#batch-submission","text":"To run reductions in batch for a specific target and date on all four channels: cdw cp /work/03946/hetdex/maverick/run_lrs2/runlrs2general . runlrs2general DATE TARGET_NAME You will receive a job number upon submission. Monitor logs such as reductionlrs2daily.oXXXXXX for progress. See also: TACC Overview , CLI Usage","title":"Batch submission"},{"location":"user-guide/cli/","text":"CLI Usage This page documents the panacea-lrs2 command-line interface. Run help: panacea-lrs2 -h Common options (as implemented in the current CLI): usage: panacea-lrs2 [-h] [-d DATE] [-s SIDES] [-o OBJECT] [-uf] [-cf] [-cw CENTRAL_WAVE] [-wb WAVELENGTH_BIN] [-re] [-rf] [-rd] [--baseraw BASERAW]usage: panacea-lrs2 [-h] [-d DATE] [-s SIDES] [-o OBJECT] [-uf] [-cf] [-cw CENTRAL_WAVE] [-wb WAVELENGTH_BIN] [-re] [-rf] [-rd] [--baseraw BASERAW] options: -h, --help show this help message and exit -d, --date DATE Observation date YYYYMMDD -s, --sides SIDES Comma-separated channels or just a single channel -o, --object OBJECT Substring to match OBJECT header (omit to reduce all) -uf, --use_flat Use internal flat (FLT) instead of twilight flats -cf, --correct_ftf Enable additional fiber-to-fiber correction using sky emission -cw, --central_wave CENTRAL_WAVE Center wavelength (\u00c5) for collapsed image -wb, --wavelength_bin WAVELENGTH_BIN Half-width (\u00c5) of collapse window around center -re, --reduce_eng Use engineering (ENG) exposures instead of SCI -rf, --reduce_flt Use flat (FLT) frames as science -rd, --reduce_drk Use dark (DRK) frames as science --baseraw BASERAW Base directory containing LRS2 raw data (tarballs). Overrides the built-in default. Notes: - The collapse window is [central_wave - wavelength_bin, central_wave + wavelength_bin]. If central_wave is not provided, the center of the channel\u2019s wavelength range is used. - If you provide central_wave, do not try to reduce multiple channels as that should be done for one channel at a time - Flags using action \"count\" (-uf, -cf, -re, -rf, -rd) are treated as booleans: include the flag to enable the behavior. Examples: - Reduce only the orange channel for a given night (suggested usage): panacea-lrs2 -d 20181108 -s orange Reduce all channels and enable ftf correction: panacea-lrs2 -d 20181108 -cf Reduce all four channels using internal flats panacea-lrs2 -d 20181108 -uf See also: Quickstart , Running on TACC","title":"CLI Usage"},{"location":"user-guide/cli/#cli-usage","text":"This page documents the panacea-lrs2 command-line interface. Run help: panacea-lrs2 -h Common options (as implemented in the current CLI): usage: panacea-lrs2 [-h] [-d DATE] [-s SIDES] [-o OBJECT] [-uf] [-cf] [-cw CENTRAL_WAVE] [-wb WAVELENGTH_BIN] [-re] [-rf] [-rd] [--baseraw BASERAW]usage: panacea-lrs2 [-h] [-d DATE] [-s SIDES] [-o OBJECT] [-uf] [-cf] [-cw CENTRAL_WAVE] [-wb WAVELENGTH_BIN] [-re] [-rf] [-rd] [--baseraw BASERAW] options: -h, --help show this help message and exit -d, --date DATE Observation date YYYYMMDD -s, --sides SIDES Comma-separated channels or just a single channel -o, --object OBJECT Substring to match OBJECT header (omit to reduce all) -uf, --use_flat Use internal flat (FLT) instead of twilight flats -cf, --correct_ftf Enable additional fiber-to-fiber correction using sky emission -cw, --central_wave CENTRAL_WAVE Center wavelength (\u00c5) for collapsed image -wb, --wavelength_bin WAVELENGTH_BIN Half-width (\u00c5) of collapse window around center -re, --reduce_eng Use engineering (ENG) exposures instead of SCI -rf, --reduce_flt Use flat (FLT) frames as science -rd, --reduce_drk Use dark (DRK) frames as science --baseraw BASERAW Base directory containing LRS2 raw data (tarballs). Overrides the built-in default. Notes: - The collapse window is [central_wave - wavelength_bin, central_wave + wavelength_bin]. If central_wave is not provided, the center of the channel\u2019s wavelength range is used. - If you provide central_wave, do not try to reduce multiple channels as that should be done for one channel at a time - Flags using action \"count\" (-uf, -cf, -re, -rf, -rd) are treated as booleans: include the flag to enable the behavior. Examples: - Reduce only the orange channel for a given night (suggested usage): panacea-lrs2 -d 20181108 -s orange Reduce all channels and enable ftf correction: panacea-lrs2 -d 20181108 -cf Reduce all four channels using internal flats panacea-lrs2 -d 20181108 -uf See also: Quickstart , Running on TACC","title":"CLI Usage"},{"location":"user-guide/configuration/","text":"Configuration and Packaged Resources Panacea bundles instrument configuration resources with the Python package under panacea/lrs2_config. At runtime, code locates these files via importlib.resources, so you normally do not need to manage absolute paths. What\u2019s inside lrs2_config repository 1) IFU fiber-center mappings (used for spatial geometry) - LRS2_B_UV_mapping.txt - LRS2_B_OR_mapping.txt - LRS2_R_NR_mapping.txt - LRS2_R_FR_mapping.txt Where used: routine.get_ifucenfile(side, amp) loads these and returns (x, y) fiber coordinates for each amplifier half. 2) DAR reference tables (used to model differential atmospheric refraction) - dar_BL.dat - dar_BR.dat - dar_RL.dat - dar_RR.dat Where used: routine.big_reduction reads the channel-appropriate table (BL for uv, BR for orange, RL for red, RR for farred) to build wavelength-dependent DAR offsets (xoff, yoff). 3) Arc line lists (used for wavelength calibration) - lines_uv.dat - lines_orange.dat - lines_red.dat - lines_farred.dat Where used: run_panacea.py opens lines_{channel}.dat and passes them to wavelength.get_wavelength_from_arc, which detects arc peaks and fits dispersion per fiber. 4) Average response functions (packaged medians; used if no user-provided response) - response_uv.fits - response_orange.fits - response_red.fits - response_farred.fits Where used: run_panacea.py loads response_{channel}.fits when building the per-channel CALS bundle and to optionally scale spectra. Note: There are also response_BL.dat, response_BR.dat, response_RL.dat, response_RR.dat text files present; these are legacy and not currently read by the quicklook path. 5) Skyline masks (used to suppress skylines during source finding and statistics) - orange_skylines.dat - red_skylines.dat - farred_skylines.dat - uv channel: there is no uv_skylines.dat in this tree. The code tolerates a missing file and applies a small built-in mask for uv in utils.mask_skylines_cosmics. Where used: utils.mask_skylines_cosmics loads f\"{channel}_skylines.dat\" if present. 6) Focal-plane geometry (used for astrometric mapping) - fplane.txt Where used: Astrometry/FPlane classes in astrometry.py. The quicklook reduction passes this into routine.big_reduction so reconstructed cubes can be mapped into IFU plane coordinates and (if headers allow) sky coordinates. 7) Fiber_Locations (reference fiber traces; pick closest date) - Directory structure: Fiber_Locations/YYYYMMDD/fiber_loc_{specid} {ifuslot} {ifuid}_{amp}.txt - Present dates: 20160724, 20170305, 20181108 with files for amps LL, LU, RL, RU and specids 501/502/503. Where used: trace.get_trace_reference finds the closest-in-time subdirectory and loads the matching file; trace.get_trace uses it to seed and stabilize trace finding. 8) Additional calibration aids currently packaged but not used by run/routine - ftf_BL.fits, ftf_BR.fits, ftf_RL.fits, ftf_RR.fits (candidate fiber-to-fiber templates) - uv_wavelength.fits, orange_wavelength.fits, red_wavelength.fits, farred_wavelength.fits (candidate wavelength priors) Overriding the packaged defaults - Within Python: You can open and use your own files, but for the CLI quicklook, the simplest \u201coverride\u201d is to place a file with the same name in the installed package data. This is easiest in an editable/development install. Example for replacing the ORANGE arc lines: cp my_lines_orange.dat src/panacea/lrs2_config/lines_orange.dat (Reinstall or rebuild the package as needed.) File formats (summary) - Mapping files (LRS2_ mapping.txt): ASCII with columns [fiber_id, x, y, ...]; routine.get_ifucenfile reads columns 0\u20132 (id, x, y) and splits by amplifier. - DAR tables (dar .dat): ASCII with header; columns wave, x_0, y_0 (in IFU pixel units) sampled across the channel bandpass. - Arc line lists (lines_ .dat): ASCII, whitespace-separated with optional comments (#). utils.read_arc_lines parses the first four columns as [wavelength, approx_x, relative_intensity, name]. - Response FITS (response_ .fits): Primary HDU contains a 2-row array [wavelength, response] used to scale spectra. - Skyline masks (*_skylines.dat): One wavelength per line; utils.mask_skylines_cosmics masks \u00b1~6 \u00c5 around each listed wavelength. - fplane.txt: Whitespace-separated columns describing IFU slot, positions, spectrograph id/slot, ifuid, rotation, plate scale. astrometry.FPlane expects 8 fields per row and skips comments (#). - Fiber_Locations files: Per-amp reference arrays with per-fiber [column_index, dead_flag]; trace.get_trace_reference reads them as text. See also - CLI Usage: ../user-guide/cli.md - API Reference for utils.get_config_file and related helpers: ../api/index.md","title":"Configuration"},{"location":"user-guide/configuration/#configuration-and-packaged-resources","text":"Panacea bundles instrument configuration resources with the Python package under panacea/lrs2_config. At runtime, code locates these files via importlib.resources, so you normally do not need to manage absolute paths. What\u2019s inside lrs2_config repository 1) IFU fiber-center mappings (used for spatial geometry) - LRS2_B_UV_mapping.txt - LRS2_B_OR_mapping.txt - LRS2_R_NR_mapping.txt - LRS2_R_FR_mapping.txt Where used: routine.get_ifucenfile(side, amp) loads these and returns (x, y) fiber coordinates for each amplifier half. 2) DAR reference tables (used to model differential atmospheric refraction) - dar_BL.dat - dar_BR.dat - dar_RL.dat - dar_RR.dat Where used: routine.big_reduction reads the channel-appropriate table (BL for uv, BR for orange, RL for red, RR for farred) to build wavelength-dependent DAR offsets (xoff, yoff). 3) Arc line lists (used for wavelength calibration) - lines_uv.dat - lines_orange.dat - lines_red.dat - lines_farred.dat Where used: run_panacea.py opens lines_{channel}.dat and passes them to wavelength.get_wavelength_from_arc, which detects arc peaks and fits dispersion per fiber. 4) Average response functions (packaged medians; used if no user-provided response) - response_uv.fits - response_orange.fits - response_red.fits - response_farred.fits Where used: run_panacea.py loads response_{channel}.fits when building the per-channel CALS bundle and to optionally scale spectra. Note: There are also response_BL.dat, response_BR.dat, response_RL.dat, response_RR.dat text files present; these are legacy and not currently read by the quicklook path. 5) Skyline masks (used to suppress skylines during source finding and statistics) - orange_skylines.dat - red_skylines.dat - farred_skylines.dat - uv channel: there is no uv_skylines.dat in this tree. The code tolerates a missing file and applies a small built-in mask for uv in utils.mask_skylines_cosmics. Where used: utils.mask_skylines_cosmics loads f\"{channel}_skylines.dat\" if present. 6) Focal-plane geometry (used for astrometric mapping) - fplane.txt Where used: Astrometry/FPlane classes in astrometry.py. The quicklook reduction passes this into routine.big_reduction so reconstructed cubes can be mapped into IFU plane coordinates and (if headers allow) sky coordinates. 7) Fiber_Locations (reference fiber traces; pick closest date) - Directory structure: Fiber_Locations/YYYYMMDD/fiber_loc_{specid} {ifuslot} {ifuid}_{amp}.txt - Present dates: 20160724, 20170305, 20181108 with files for amps LL, LU, RL, RU and specids 501/502/503. Where used: trace.get_trace_reference finds the closest-in-time subdirectory and loads the matching file; trace.get_trace uses it to seed and stabilize trace finding. 8) Additional calibration aids currently packaged but not used by run/routine - ftf_BL.fits, ftf_BR.fits, ftf_RL.fits, ftf_RR.fits (candidate fiber-to-fiber templates) - uv_wavelength.fits, orange_wavelength.fits, red_wavelength.fits, farred_wavelength.fits (candidate wavelength priors) Overriding the packaged defaults - Within Python: You can open and use your own files, but for the CLI quicklook, the simplest \u201coverride\u201d is to place a file with the same name in the installed package data. This is easiest in an editable/development install. Example for replacing the ORANGE arc lines: cp my_lines_orange.dat src/panacea/lrs2_config/lines_orange.dat (Reinstall or rebuild the package as needed.) File formats (summary) - Mapping files (LRS2_ mapping.txt): ASCII with columns [fiber_id, x, y, ...]; routine.get_ifucenfile reads columns 0\u20132 (id, x, y) and splits by amplifier. - DAR tables (dar .dat): ASCII with header; columns wave, x_0, y_0 (in IFU pixel units) sampled across the channel bandpass. - Arc line lists (lines_ .dat): ASCII, whitespace-separated with optional comments (#). utils.read_arc_lines parses the first four columns as [wavelength, approx_x, relative_intensity, name]. - Response FITS (response_ .fits): Primary HDU contains a 2-row array [wavelength, response] used to scale spectra. - Skyline masks (*_skylines.dat): One wavelength per line; utils.mask_skylines_cosmics masks \u00b1~6 \u00c5 around each listed wavelength. - fplane.txt: Whitespace-separated columns describing IFU slot, positions, spectrograph id/slot, ifuid, rotation, plate scale. astrometry.FPlane expects 8 fields per row and skips comments (#). - Fiber_Locations files: Per-amp reference arrays with per-fiber [column_index, dead_flag]; trace.get_trace_reference reads them as text. See also - CLI Usage: ../user-guide/cli.md - API Reference for utils.get_config_file and related helpers: ../api/index.md","title":"Configuration and Packaged Resources"},{"location":"user-guide/examples/","text":"Examples This page collects short, copy-pasteable examples for common Panacea tasks. Prerequisites - Install Panacea and dev extras for the CLI and examples: pip install .[dev] CLI: Reduce a small dataset - Run the pipeline on a directory or tarball. Use --help to see options. panacea-lrs2 --help panacea-lrs2 --input /path/to/lrs2/night/ --channel red --output ./out Tips Use --config to point to a YAML config if needed. Logs will indicate which frames are used and where products are written. Python API: Smooth a sky spectrum continuum import numpy as np from panacea.routine import fit_response_cont wv = np.linspace(4000.0, 5000.0, 501) # synthetic spectrum with a gentle slope and a narrow line sky = 1.0 + 0.001 * (wv - wv.mean()) sky[250] += 0.8 cont = fit_response_cont(wv, sky, fil_len=21) Python API: Safe numerical utilities import numpy as np from panacea.utils import safe_division, build_weight_matrix num = np.array([1.0, 2.0, -3.0, 4.0]) denom = np.array([1.0, 0.0, np.inf, 1e-12]) out = safe_division(num, denom, eps=1e-8, fillval=0.0) x = np.array([0.0, 1.0, 0.0, 1.0]) y = np.array([0.0, 0.0, 1.0, 1.0]) W = build_weight_matrix(x, y, sig=0.75) Python API: Astrometry quicklook from panacea.astrometry import Astrometry astro = Astrometry(ra0=150.0, dec0=2.2, pa=0.0, x0=0.0, y0=0.0) ra, dec = astro.get_ifuslot_ra_dec('054') See also - API reference: ../api/index.md - CLI Usage: ./cli.md - Configuration: ./configuration.md","title":"Examples"},{"location":"user-guide/examples/#examples","text":"This page collects short, copy-pasteable examples for common Panacea tasks. Prerequisites - Install Panacea and dev extras for the CLI and examples: pip install .[dev] CLI: Reduce a small dataset - Run the pipeline on a directory or tarball. Use --help to see options. panacea-lrs2 --help panacea-lrs2 --input /path/to/lrs2/night/ --channel red --output ./out Tips Use --config to point to a YAML config if needed. Logs will indicate which frames are used and where products are written. Python API: Smooth a sky spectrum continuum import numpy as np from panacea.routine import fit_response_cont wv = np.linspace(4000.0, 5000.0, 501) # synthetic spectrum with a gentle slope and a narrow line sky = 1.0 + 0.001 * (wv - wv.mean()) sky[250] += 0.8 cont = fit_response_cont(wv, sky, fil_len=21) Python API: Safe numerical utilities import numpy as np from panacea.utils import safe_division, build_weight_matrix num = np.array([1.0, 2.0, -3.0, 4.0]) denom = np.array([1.0, 0.0, np.inf, 1e-12]) out = safe_division(num, denom, eps=1e-8, fillval=0.0) x = np.array([0.0, 1.0, 0.0, 1.0]) y = np.array([0.0, 0.0, 1.0, 1.0]) W = build_weight_matrix(x, y, sig=0.75) Python API: Astrometry quicklook from panacea.astrometry import Astrometry astro = Astrometry(ra0=150.0, dec0=2.2, pa=0.0, x0=0.0, y0=0.0) ra, dec = astro.get_ifuslot_ra_dec('054') See also - API reference: ../api/index.md - CLI Usage: ./cli.md - Configuration: ./configuration.md","title":"Examples"}]}